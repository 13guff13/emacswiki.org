Please note that the stupid [http://www.emacswiki.org/alex/2009-07-29_Best_Essays_Spam Best Essays] link spam has reached Emacs Wiki.
These guys try to hide their nefarious spam using apparently legitimate usernames, rollbacks, and links to the old revisions.
The stupid spammers don't know that the spammed revisions will expire within two weeks, so the net result is that they're
vandalising the wiki.

You might consider watching [Self:action=rc;showedit=1;rollback=1 RecentChanges including minor edits and rollbacks] if you want to help
revert this vandalism. The edits I've looked at today all seemed to come from the IP number 119.111.124.194.
-- AlexSchroeder

[new]
We've reached the end of 2009 and the stupid essay spam is still as strong as ever. Any ideas on how to handle it without inconveniencing regular users? -- Alex

[new:DrewAdams:2009-12-24 23:28 UTC]
Sorry, no idea. It sure is a PITA, though. -- DrewAdams

[new]

Why not add a custom regex edit rule for this? Most of the domains contain the word "papers", because it's a SEO keyword for them. This world (and other words like "essay") could be banned from URLs, so if a new edit contains an URL and it contains the word "papers" then it is not allowed to proceed.

The word papers is not too frequent in legitmate URLs, so this ban could be employed for a trial period without bothering legitimate users to see its effectiveness. 


[new:DrewAdams:2009-12-25 19:43 UTC]
They're really stepping up the spam attacks -- I suppose for the holidays. The latest flurry, from the "eluneart" URL, seems to be from a bot: the edit times are close together. -- DrewAdams

[new]

What's with the CAPTCHA barrier? Have they broken reCaptcha?

[new]
Captchas are known to be broken at least on blogs.

I use wp-hashcash for my blog (which I do not have time to use now...).
It seems to work well.

    http://en.wikipedia.org/wiki/Hashcash

The strategy in hashcash is to let the client with javascript compute something that takes time (and then of course include the result in what is sent back and check that it is correct).
That works well since spammers do not have the time to wait.
Do you think this can be used here too?

[new]
I think the essay and paper spam is written by actual people. It doesn't just add spam to a page – they're actively trying to hide it by producing semi-legible text. As for the gulidova.client.kv.chereda.net spam – I looked at AlexSchroederConfigPyrobombus and noticed that they spammed the page, and then reverted their own spam, probably because it wasn't being linked? They didn't revert their own spam on EmacsNiftyTricks, for example.

If you look at the list of _all_ changes (not just the latest one for each page), you'll see that gulidova.client.kv.chereda.net spammed the site on 2009-12-25 from 17:37 to 17:55 with about an edit per minute. They had lots of time. -- Alex

[new:DrewAdams:2009-12-26 19:26 UTC]
Guess you're right. But then "they had lots of time" in both senses -- time to kill, apparently. Spending 20 minutes doing that by hand, at a page per minute -- they gotta ''love'' it! -- Drew

[new]
But then I suppose some intervention on the human level is necessary.
Registration of users?
Email confirmation of edits? -- LennartBorgman

[new]
Alex, one of the things we did on another (now defunct) wiki was to set up the robots.txt to tell google to exclude history pages. That and nofollow to the history pages. It will make the linkspam worthless, which won't necessarily reduce the dumber spammers, but it will at least render them impotent. That and 2 weeks expiry seems excessive. -- RyanDavis

[new]

For dumber spammers there could be an explicit warning on the edit page that their spam is not indexed on the history pages.
